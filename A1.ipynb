{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duhhz/UTS_ML2019_ID11702293/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4MfTxEWfvVN",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Generative Adversarial Nets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0C6noSjfvVO",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj99n0Kfr0uo",
        "colab_type": "text"
      },
      "source": [
        "This report reviews the paper “Generative Adversarial Nets” by Ian Goodfellow. The report will be commenting, critiquing and making suggestions on the content, innovation, technical quality, application and presentation of the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCDDAcm2fvVO",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWaB4G5KfvVP",
        "colab_type": "text"
      },
      "source": [
        "Generative models describe how a dataset is generated, in terms of a probabilistic model, and by sampling from this model we are able to generate new data (Foster 2019, p. 1). This paper introduces Generative Adversarial Networks (GANs), which is an algorithm of generative models proposed by Ian Goodfellow in 2014. In particular it falls under the category of generative models that work via maximum likelihood estimation, which involves estimating the probability distribution using parameters which we choose in order to maximise the likelihood of the training data. Other models that fall under this category are shown in Figure 1.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1p1qXuWdwE1hywAJzEoHRuuYsfxUYbNR2)\n",
        "\n",
        "*Figure 1: Taxonomy of generative models*\n",
        "\n",
        "Goodfellow (2014) goes into detail of the other generative models that exist and disadvantages associated with them this is shown in Table 1. GANs was designed to avoid the disadvantages present in other generative models. Goodfellow focuses on two particular disadvantages of other models; the use of Markov Chain Monte Carlo and the difficulty of leveraging the benefits of piecewise linear units. Markov Chain is used to estimate difficult partition functions and their gradients; however, an approximation can pose a significant problem for learning algorithms such as the speed of generators as well as how closely the Markov Chain can approximate to the training data.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1AiB2eKqhg7jG3ptPGkxUGJCL2KXTPsLU)\n",
        "\n",
        "*Table 1: Summary of the challenges in generative modelling*\n",
        "\n",
        "Instead generative adversarial networks directly and explicitly estimate a probabilistic model. It comprises of two components, a discriminator (D) which distinguishes between training data and generated samples and the other, a generator (G) which ‘creates samples intended to come from the same distribution as the training data’ (Goodfellow 2016, p. 17) to fool the discriminator. Each model takes turns at improving until they reach a point where they can no longer improve since the generated data represents the training data and the discriminator can not differentiate between the two distributes (D(x) = ½).\n",
        "\n",
        "While GANs avoids some of the disadvantages of other generative models, it also creates new disadvantages. The two main disadvantages are that there are ‘no explicit representation’ (Goodfellow 2014, p. 7) of the generator and the other is that the model can face mode collapse. Mode collapse occurs when the generator produces limited diversity of samples, this can occur because the generator focuses on a limited number of modes and hence produces a limited set of samples.\n",
        "\n",
        "The ability to generate sharp, even generative distributions better than any other model while being able to generate samples at a faster pace is important. And when considering these disadvantages of GANs, I believe the advantages outweigh the disadvantages. GANs has a large potential to enhance and open up new avenues within machine learning. The ability to learn and mimic any distribution of data and generate things such as images and audio will begin to close the gap between what machines cannot do, that humans can.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHWXGGkfvVP",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inNRsX4AfvVQ",
        "colab_type": "text"
      },
      "source": [
        "At the time, deep learning had made some big successes which revolved around the use of backpropagation and dropout algorithms. However, generative models had less success due to difficulties identified earlier. As a result, other models produced poor sample qualities such as blurry images or involved slow and inefficient sequential generation. This required a new generative model that addressed these issues.\n",
        "\n",
        "The creative idea was to create a generative model that implemented backpropagation and dropout algorithms to avoid the disadvantages of other models. GANs avoids the use of Markov chains by training the generator with random noise through a multilayer perceptron and generates sample data (fake data) which inputs into the discriminator with training data (real data). The use of random noise meant that the generator does not sample from the training data and so if given enough time and capacity, could theoretically converge to the training data. This effectively allows the model to produce its realistic data and trains the model without bias from the training data.\n",
        "\n",
        "These inputs go through the discriminators multilayer perceptron and the discriminator determines whether the data was generated or from the training data and outputs the probability D(x) that the input came from training rather than samples. The generator is then improved through backpropagation, with involves using the error rates for each parameter in the multilayer perceptron. The input and output process of the GAN model is shown in Figure 2. Avoiding Markov Chains also allows GANs to better leverage piecewise linear units since it does not require performing a step of a generative Markov chain for feedback. This difference from other models makes GANs more efficient and generate samples faster than other models.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1sL_W1I_wxk3M5UprhnfhWHPzfObcbcFT)\n",
        "\n",
        "*Figure 2: A graphical representation of the GAN model*\n",
        "\n",
        "\n",
        "GANs works as a ‘minmax two-player game’ (Goodfellow 2014, p. 3) whereby each model is driven to improve and achieve their individual goal. The generator wants to minimise the probability that discriminator outputs for the samples such that D(G(z)) is close to 1 which means the discriminator is fooled into thinking G(z) is real.  Whereas the discriminator wants to maximise the probability such that D(x) is close to 1 (labels real data correctly) and D(G(z)) is close to zero. The value function that describes this is shown in Equation 1.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1ZkXF7UmygbGxkopyLtJ4Of_oGfdUwYsE)\n",
        "\n",
        "*Equation 1: The Value Function V(G,D) that describes the two-player minmax game*\n",
        "\n",
        "However, in practice this does not work well, as shown in Figure 3 below the blue line represents this scenario. Early in learning, the generator is poor and the discriminator can reject samples with high confidence, and later in training when samples are already good, we have higher gradients. Which lead to another creative solution which involves rather than minimising log(1-D(G(z))) we train the generator to maximise log(D(G(z))), this will result in stronger gradients early in learning while maintaining the same objective of fooling the discriminator, this is represented by the green line.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1ztQbAHDKnB9iCLz3MwgCeYfKPKIHBiV-)\n",
        "\n",
        "*Figure 3: A figure of the gradient signals based with respect to the D(G(z))*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rRSlZw-fvVR",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KbXwWxofvVS",
        "colab_type": "text"
      },
      "source": [
        "The technical development of the generative model was of high quality. In the section of theoretical results, the authors presented sufficient supporting evidence in a logical order to prove their theorem. The section starts off by describing the algorithm for training the adversarial network in pseudocode. It then goes on to show that the minmax game has a global optimum when probability distribution of the generator matches the probability distribution of the training data. It then lastly shows how the algorithm optimises the equation stated in the paper and reaches the optimum. \n",
        "\n",
        "The authors supported their theory by conducting an experiment, Goodwell compared the GAN model’s performance against three other generative models on three datasets. They described the steps taken within the experimental well.  However, the performance between models was compared by ‘estimating the probability of the test set under pg’ (Goodwell 2014, p. 5) and reporting the log-likelihood. The authors admit themselves that this method of comparison was unfair as it had somewhat ‘high variance and does not perform well in high dimensional spaces’ (Goodwell 2014, p. 6) but was the best method to their knowledge. The difficulty in comparing generative models to each other occurs because certain models may produce good likelihood but generate bad samples and vice versa. And so, while the results were not fairly compared to other models, it was understandable considering the circumstances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdmIi6OMfvVS",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IlXaF08fvVT",
        "colab_type": "text"
      },
      "source": [
        "While the paper does not explicitly propose a certain application domain, the experiment was used on image datasets and I do believe GANs is appropriate for dealing with generating images. This is because of the nature of images whereby there is a high dimension of data as well as the data being unlabelled makes it suitable for unsupervised learning such as GANs. \n",
        "\n",
        "Further developments of research work will involve in improving the biggest downfalls of the model. Which are the difficulties with quantitatively evaluate of the model’s performance, as well as solutions to issues where the model does not converge, such as mode collapse. Addressing these two issues will improve the limitations of GANs and maintain it as a relevant model in machine learning.\n",
        "\n",
        "The most obvious other application domains of GANs would involve audio and video. This is because they share common elements with images in that they contain a high dimension of data and suitable for unsupervised learning. GANs could be trained to generate music of a specific genre however, can be challenging as it ‘does not consider the discrete property of music elements’ (Hong 2019, p. 26). \n",
        "\n",
        "What I found interesting in this report was how the paper used the analogy of game theory to describe the model which was clever due to making the concept behind the GAN model easier to understand but also creative as it brought in an existing theory to solve a modern problem. It also made me consider the consequences of something as powerful as this model, especially as it improves. A technology that can be used to generate realistic images that can fool even humans can cause harm if used improperly. Another research idea would look into how to ensure the technology is used ethically and mechanisms to ensure safety.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LBWK_D5fvVU",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucm0h6Q6fvVU",
        "colab_type": "text"
      },
      "source": [
        "Overall the paper was structured in a logical manner which helped with following the argument of the paper. One thing that could be improved on this matter would have been to include a section early on that described the paper’s organisation, even if it was a relatively short paper. The depth of the paper started off well, it went into a considerable amount of detail with related work, explaining the adversarial network, and showing the theoretical proofs. However, towards the end, the discussion on the experimental results was lacking as well as on the advantages and disadvantages. Which could have been explained in more detail rather than a table summary with a few short paragraphs. The paper could have also been more attractive if the author had included an additional diagram to explain the adversarial network. While there was a diagram to show how the two model converge, a diagram to represent the algorithm and how it works in terms of the inputs and outputs would have improved clarity in the paper. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc2z6wZJsvrv",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoRkG7UIswED",
        "colab_type": "text"
      },
      "source": [
        "In conclusion, Generative Adversarial Networks is a different form of generative models which avoided many disadvantages of other models. It involves a generator and discriminator model that plays against each other and improves to achieve their individual goals. The model is truly one of the more innovative and clever works in machine learning that will have a large impact on how we approach machine learning problems we couldn’t solve before. The paper was written with a high technical quality through its thorough proofs of theorems as well as acceptable experimental results. GANs has a huge potential to be applied in many domain applications such as audio and video but has many challenges ahead of it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIUMmLW0fvVV",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Foster, D. 2019, Generative Deep Learning, 1st edn, O’Reilly Media, California.\n",
        "\n",
        "Goodfellow, I., 2016. ‘NIPS 2016 tutorial: Generative adversarial networks’, arXiv preprint arXiv:1701.00160.\n",
        "\n",
        "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014, ‘Generative adversarial nets’, Advances in neural information processing systems pp. 2672-2680.\n",
        "\n",
        "Hong, Y. Hwang, U. Yoo, J. Yoon, S. 2019, ‘How Generative Adversarial Networks and Their Variants Work: An Overview’, ACM Computing Surveys, vol. 52, no. 1 pp. 1-43.\n",
        "\n"
      ]
    }
  ]
}