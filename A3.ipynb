{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duhhz/UTS_ML2019_ID11702293/blob/master/A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv7vJQxAlzep",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This report will explain how ensemble methods work to create diverse classifiers and how they combine the decisions from individual classifiers to contribute to the ensemble’s answer. Specifically, this report will look into the two main approaches of ensemble learning: bagging and boosting and discuss their advantages and disadvantages. Lastly a plan to address issues related to these issues will be described and explained how this method can outperform traditional methods.\n",
        "\n",
        "# Ensemble Learning\n",
        "Ensemble learning is a process where a group of models are used to predict an outcome by combining their predictions and voting on a majority. This idea follows the same logic of the saying ‘wisdom of the crowd’ which states that the opinion of many individuals is better than a single expert. For example, a single classifier might have the best individual accuracy for predictions, but still make errors. By including other diverse models these errors can be reduced since the error is averaged out. Errors made from models are made of three components: bias, variance and irreducible error. The first two components we can control to reduce the overall error. Bias is an error made due to false assumptions made in the learning phase, whereas variance is an error made because the model is sensitive to changes in training data. Often these two components have a trade-off relationship, where classifiers with low variance have high bias vice versa (Polikar 2012).\n",
        "\n",
        "#Diversity\n",
        "In ensemble models, the combination of classifiers is only beneficial if they disagree, since if they agreed on every classification, this results in no difference from having a single best classifier and errors would not be reduced. The measure of disagreement is referred to as the diversity of ensemble models (Melville 2003). Diversity of models can be achieved in several ways such as using different classifier algorithms or training data sets. The use of different classifiers would result in different models and errors made since, each classifier has a different approach to classifying data. With different training data sets, each classifier is trained on different data which would produce different models and errors. These two ideas to increase the diversity of models are not always possible or require large amount of additional work, for example, we may not be able to train on different data when the there is a limited amount of data.\n",
        "\n",
        "#Establish weights of individual classifiers\n",
        "Once an ensemble has multiple diverse models, the next issue involves deciding how to combine their decisions together. A simple approach to this would be to take a majority vote, that is classify based on which label received the highest number of votes. However, this approach may not always provide the best accuracy, if there was a classifier that is more likely to predict correctly than others then providing more weight to those decisions can improve the overall performance. However, without the knowledge of which classifier performs better this becomes a difficult task. A method to determine these weights involve using the performance on a separate validation dataset as an estimation of the classifier’s performance.\n",
        "\n",
        "#Bagging (Bootstrap Aggregating)\n",
        "The Bagging approach to ensemble involves training each classifier on a set of m training data, which is selected randomly with replacement from the original training dataset of size m. Each training set is referred to as a bootstrap replicate of the original set. Since each classifier is trained independently from each other on different sets of training data, they become diverse. \n",
        "The main disadvantages of this approach occur when each classifier is trained on a smaller training set overall, the bias of each individual classifier increases. The approach is also more expensive than using one classifier as there are computational costs involved in splitting the dataset and training multiple classifiers. It also loses its interpretability since the final classifier is not a tree. However, the advantages of using the smaller training set is that it avoids overfitting, since each classifier is not exposed to the whole training set which results in reducing error from variance. In practice it has been found that the trade-off between decreasing variance and increasing bias yields a more accurate ensemble overall. Another advantage to bagging is its ability to work with small datasets since the approach uses bootstraps of the original training data multiple times it maximises the use of a small dataset. \n",
        "\n",
        "#Boosting \n",
        "The Boosting approach to ensemble also combines weak classifiers like bagging, however differs in how the training data is given to each classifier to train on. Instead of producing bootstraps randomly, the boosting ensemble compensates for the weakness of the base classifier by concentrating the training data on instances of misclassification by other classifiers. For example, the first classifier would be trained and the errors made would be used to train the second classifier, with the last classifier training on data that the first two disagree on. The final boosted classifer combines the votes of each classifier which is calculated by the weighted majority vote based on each classifer’s accuracy. \n",
        "\n",
        "This places more weight on examples that are harder to classify and increases the accuracy of the ensemble and decreases the amount of bias. While boosting has shown to be accurate, it does not handle insufficient data as well as Bagging and so will struggle when there is missing or limited available data. The approach is also prone to overfitting since the approach focuses on errors on one dataset, it may not perform the same on another dataset.\n",
        "\n",
        "# My approach\n",
        "Conventional methods have shown that diverse classifiers can be created by training different classifiers or training on different sets of data. The proposed approach will attempt to create diverse classifiers by adopting a similar approach to bagging, whereby the original dataset is bootstrapped into smaller training datasets. However, rather than randomly replacing data, the replacement will be based on whole features, where each dataset has a different combination of features.  This approach therefore relies on having multiple features, which is an increasingly common occurrence with high dimension of data. \n",
        "\n",
        "By bagging based on features, classifiers will be trained on different combinations of features and therefore be diverse. This creates a problem since a certain feature may be important in determining the classification, and so many classifiers may have a very low accuracy. To deal with this problem, weights will need to be applied to the individual classifiers, which can be learned using traditional methods of using the classifier’s individual accuracy as a weight. \n",
        "What this approach allows is for classifiers to specialise in predicting labels based on specific combination of features. \n",
        "\n",
        "#References\n",
        "Polikar, R. 2012, Ensemble Learning, Ensemble Machine Learning: Methods and Applications, Rowan University, Glassboro.\n",
        "Melville, P. 2003, ‘Creating Diverse Ensemble Classifiers’, PhD Thesis, The University of Texas at Austin, Austin.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xiVCSkPl3Fw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}